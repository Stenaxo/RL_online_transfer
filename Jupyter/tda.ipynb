{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorma/RL_online_transfer/env/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lorma/RL_online_transfer/env/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lorma/RL_online_transfer/env/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lorma/RL_online_transfer/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lorma/RL_online_transfer/env/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse 12 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import csv\n",
    "from torchvision.models import resnet18\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class which create a custom dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset creation\n",
    "\n",
    "        Parameters:\n",
    "        --------------------------------\n",
    "        images_paths : str\n",
    "            paths for each image\n",
    "        labels : int\n",
    "            label for each images,\n",
    "        transform : torchvision.transforms.Compose\n",
    "            transformation applied to the tensors\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = (\n",
    "            F.one_hot(torch.tensor(label), num_classes=2).squeeze().to(torch.float32)\n",
    "        )\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ConvertToRGB(object):\n",
    "    \"\"\"\n",
    "    Class which convert images in 3 canal (Red, Green, Blue)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Convert images in RGB\n",
    "\n",
    "        Returns :\n",
    "        -------------\n",
    "        img : tensor\n",
    "            images with 3 canals\n",
    "        \"\"\"\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        ConvertToRGB(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        #transforms.Lambda(lambda x: torch.flatten(x))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "with open(os.path.join('.', \"test/image_list.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Créer des listes pour stocker les chemins d'image et les étiquettes\n",
    "image_paths_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Parcourir chaque ligne du fichier texte et extraire les informations\n",
    "for line in lines:\n",
    "    line = line.strip().split(\" \")\n",
    "    if line[1] == \"0\" or line[1] == \"1\": #Commentaire ici pour avoir les 12\n",
    "        image_paths_test.append(\"test/\" + line[0])\n",
    "        labels_test.append(int(line[1]))\n",
    "df_test = CustomDataset(image_paths_test, labels_test, transform=transformation)\n",
    "# DataLoader pour le CustomDataset\n",
    "dataloader = DataLoader(df_test, batch_size=1024, shuffle=True)\n",
    "#for images, labels in dataloader:\n",
    "#    images_np = images.cpu().numpy()  # Les images sont déjà aplaties grâce à la transformation\n",
    "#   labels_np = labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant PCA pour avoir 50 composantes\n",
    "pca_50 = PCA(n_components=50)\n",
    "data_pca_50 = pca_50.fit_transform(images_np)\n",
    "\n",
    "# Appliquer t-SNE sur les données réduites\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "data_tsne = tsne.fit_transform(data_pca_50)\n",
    "\n",
    "# Appliquer UMAP sur les données réduites\n",
    "umap_model = UMAP(n_components=2, random_state=42)\n",
    "data_umap = umap_model.fit_transform(data_pca_50)\n",
    "\n",
    "# Créer des DataFrame avec les données réduites\n",
    "tsne_df = pd.DataFrame(data_tsne)\n",
    "umap_df = pd.DataFrame(data_umap)\n",
    "\n",
    "# Afficher les données réduites avec t-SNE\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_tsne[mask, 0], data_tsne[mask, 1], label=f'Class {i}')\n",
    "plt.legend()\n",
    "plt.xlabel('t-SNE component 1 after PCA')\n",
    "plt.ylabel('t-SNE component 2 after PCA')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les données réduites avec UMAP\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_umap[mask, 0], data_umap[mask, 1], label=f'Class {i}')\n",
    "plt.legend()\n",
    "plt.xlabel('UMAP component 1 after PCA')\n",
    "plt.ylabel('UMAP component 2 after PCA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant PCA pour avoir 50 composantes\n",
    "pca_50 = PCA(n_components=50)\n",
    "data_pca_50 = pca_50.fit_transform(images_np)\n",
    "\n",
    "# Appliquer t-SNE en 3D sur les données réduites\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "data_tsne = tsne.fit_transform(data_pca_50)\n",
    "\n",
    "# Appliquer UMAP en 3D sur les données réduites\n",
    "umap_model = UMAP(n_components=3, random_state=42)\n",
    "data_umap = umap_model.fit_transform(data_pca_50)\n",
    "\n",
    "# Afficher les données réduites avec t-SNE en 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(data_tsne[mask, 0], data_tsne[mask, 1], data_tsne[mask, 2], label=f'Class {i}')\n",
    "ax.legend()\n",
    "ax.set_xlabel('t-SNE component 1 after PCA')\n",
    "ax.set_ylabel('t-SNE component 2 after PCA')\n",
    "ax.set_zlabel('t-SNE component 3 after PCA')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les données réduites avec UMAP en 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(data_umap[mask, 0], data_umap[mask, 1], data_umap[mask, 2], label=f'Class {i}')\n",
    "ax.legend()\n",
    "ax.set_xlabel('UMAP component 1 after PCA')\n",
    "ax.set_ylabel('UMAP component 2 after PCA')\n",
    "ax.set_zlabel('UMAP component 3 after PCA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pas de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction using UMAP\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "data_umap = umap.fit_transform(images_np)\n",
    "image = ['aeroplane', 'bicycle', 'bus', 'car', 'horse', \n",
    "              'knife','motorcycle', 'person', 'plant', 'skateboard','train','truck']\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_umap[mask, 0], data_umap[mask, 1], label=f'{image[i]}', s=10)\n",
    "plt.legend()\n",
    "plt.xlabel('UMAP component 1')\n",
    "plt.ylabel('UMAP component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant UMAP avec 3 composantes\n",
    "umap = UMAP(n_components=3, random_state=42)\n",
    "data_umap = umap.fit_transform(images_np)\n",
    "\n",
    "# Crée un DataFrame avec les données réduites\n",
    "umap_df = pd.DataFrame(data_umap, columns=['Component 1', 'Component 2', 'Component 3'])\n",
    "\n",
    "# Trace les données réduites en utilisant un graphique 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 1.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(umap_df['Component 1'][mask], umap_df['Component 2'][mask], umap_df['Component 3'][mask], label=f'{image[i]}', s = 10)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('UMAP component 1')\n",
    "ax.set_ylabel('UMAP component 2')\n",
    "ax.set_zlabel('UMAP component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse sur 12 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('.', \"train/image_list.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()  # [next(f) for _ in range(1000)]\n",
    "\n",
    "    # Créer des listes pour stocker les chemins d'image et les étiquettes\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir chaque ligne du fichier texte et extraire les informations\n",
    "    for line in lines:\n",
    "        line = line.strip().split(\" \")\n",
    "        image_paths.append(\"train/\" + line[0])\n",
    "\n",
    "        labels.append(int(line[1]))\n",
    "\n",
    "    # Appliquer les transformations d'image si nécessaire\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            ConvertToRGB(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            transforms.Lambda(lambda x: torch.flatten(x))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df_train = CustomDataset(image_paths, labels, transform=transform_train)\n",
    "# DataLoader pour le CustomDataset\n",
    "dataloader = DataLoader(df_train, batch_size=1024, shuffle=True)\n",
    "for images, labels in dataloader:\n",
    "    images_np = images.cpu().numpy()  # Les images sont déjà aplaties grâce à la transformation\n",
    "    labels_np = labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant PCA pour avoir 50 composantes\n",
    "pca_50 = PCA(n_components=50)\n",
    "data_pca_50 = pca_50.fit_transform(images_np)\n",
    "\n",
    "# Appliquer t-SNE sur les données réduites\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "data_tsne = tsne.fit_transform(data_pca_50)\n",
    "\n",
    "# Appliquer UMAP sur les données réduites\n",
    "umap_model = UMAP(n_components=2, random_state=42)\n",
    "data_umap = umap_model.fit_transform(data_pca_50)\n",
    "\n",
    "# Créer des DataFrame avec les données réduites\n",
    "tsne_df = pd.DataFrame(data_tsne)\n",
    "umap_df = pd.DataFrame(data_umap)\n",
    "\n",
    "# Afficher les données réduites avec t-SNE\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_tsne[mask, 0], data_tsne[mask, 1], label=f'Class {i}')\n",
    "plt.legend()\n",
    "plt.xlabel('t-SNE component 1 after PCA')\n",
    "plt.ylabel('t-SNE component 2 after PCA')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les données réduites avec UMAP\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_umap[mask, 0], data_umap[mask, 1], label=f'Class {i}')\n",
    "plt.legend()\n",
    "plt.xlabel('UMAP component 1 after PCA')\n",
    "plt.ylabel('UMAP component 2 after PCA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant PCA pour avoir 50 composantes\n",
    "pca_50 = PCA(n_components=50)\n",
    "data_pca_50 = pca_50.fit_transform(images_np)\n",
    "\n",
    "# Appliquer t-SNE en 3D sur les données réduites\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "data_tsne = tsne.fit_transform(data_pca_50)\n",
    "\n",
    "# Appliquer UMAP en 3D sur les données réduites\n",
    "umap_model = UMAP(n_components=3, random_state=42)\n",
    "data_umap = umap_model.fit_transform(data_pca_50)\n",
    "\n",
    "# Afficher les données réduites avec t-SNE en 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(data_tsne[mask, 0], data_tsne[mask, 1], data_tsne[mask, 2], label=f'Class {i}', s=5)\n",
    "ax.legend()\n",
    "ax.set_xlabel('t-SNE component 1 after PCA')\n",
    "ax.set_ylabel('t-SNE component 2 after PCA')\n",
    "ax.set_zlabel('t-SNE component 3 after PCA')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les données réduites avec UMAP en 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(data_umap[mask, 0], data_umap[mask, 1], data_umap[mask, 2], label=f'Class {i}',s = 5)\n",
    "ax.legend()\n",
    "ax.set_xlabel('UMAP component 1 after PCA')\n",
    "ax.set_ylabel('UMAP component 2 after PCA')\n",
    "ax.set_zlabel('UMAP component 3 after PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pas de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction using UMAP\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "data_umap = umap.fit_transform(images_np)\n",
    "\n",
    "# Create a DataFrame with the reduced data\n",
    "umap_df = pd.DataFrame(data_umap)\n",
    "\n",
    "# Plot the reduced data using pandas plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 11.\n",
    "    mask = labels_np == i\n",
    "    plt.scatter(data_umap[mask, 0], data_umap[mask, 1], label=f' {image[i]}', s = 10)\n",
    "plt.legend()\n",
    "plt.xlabel('t-SNE component 1')\n",
    "plt.ylabel('t-SNE component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Effectuer une réduction de dimensionnalité en utilisant UMAP avec 3 composantes\n",
    "umap = UMAP(n_components=3, random_state=42)\n",
    "data_umap = umap.fit_transform(images_np)\n",
    "\n",
    "# Crée un DataFrame avec les données réduites\n",
    "umap_df = pd.DataFrame(data_umap, columns=['Component 1', 'Component 2', 'Component 3'])\n",
    "\n",
    "# Trace les données réduites en utilisant un graphique 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(12):  # Supposant que les classes sont étiquetées de 0 à 1.\n",
    "    mask = labels_np == i\n",
    "    ax.scatter(umap_df['Component 1'][mask], umap_df['Component 2'][mask], umap_df['Component 3'][mask], label=f'{image[i]}', s = 10)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('UMAP component 1')\n",
    "ax.set_ylabel('UMAP component 2')\n",
    "ax.set_zlabel('UMAP component 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7026586532592773\n",
      "Epoch 2, Loss: 2.200211763381958\n",
      "Epoch 3, Loss: 1.4535274505615234\n",
      "Epoch 4, Loss: 0.8095605373382568\n",
      "Epoch 5, Loss: 0.7049943208694458\n",
      "Epoch 6, Loss: 0.6871241927146912\n",
      "Epoch 7, Loss: 0.7027666568756104\n",
      "Epoch 8, Loss: 0.6548987030982971\n",
      "Epoch 9, Loss: 0.5824663639068604\n",
      "Epoch 10, Loss: 0.5156244039535522\n",
      "Epoch 11, Loss: 0.41505053639411926\n",
      "Epoch 12, Loss: 0.3957947790622711\n",
      "Epoch 13, Loss: 0.3861728608608246\n",
      "Epoch 14, Loss: 0.37021663784980774\n",
      "Epoch 15, Loss: 0.3427679240703583\n",
      "Epoch 16, Loss: 0.3300860822200775\n",
      "Epoch 17, Loss: 0.3128366768360138\n",
      "Epoch 18, Loss: 0.29459574818611145\n",
      "Epoch 19, Loss: 0.28798574209213257\n",
      "Epoch 20, Loss: 0.26294928789138794\n",
      "Epoch 21, Loss: 0.25266551971435547\n",
      "Epoch 22, Loss: 0.24384705722332\n",
      "Epoch 23, Loss: 0.24188119173049927\n",
      "Epoch 24, Loss: 0.23794806003570557\n",
      "Epoch 25, Loss: 0.23206642270088196\n",
      "Epoch 26, Loss: 0.2295873761177063\n",
      "Epoch 27, Loss: 0.22767110168933868\n",
      "Epoch 28, Loss: 0.2164156436920166\n",
      "Epoch 29, Loss: 0.21316732466220856\n",
      "Epoch 30, Loss: 0.20612704753875732\n",
      "Epoch 31, Loss: 0.19558221101760864\n",
      "Epoch 32, Loss: 0.19577036798000336\n",
      "Epoch 33, Loss: 0.1964302510023117\n",
      "Epoch 34, Loss: 0.19868843257427216\n",
      "Epoch 35, Loss: 0.19990217685699463\n",
      "Epoch 36, Loss: 0.19327589869499207\n",
      "Epoch 37, Loss: 0.1908372938632965\n",
      "Epoch 38, Loss: 0.19816236197948456\n",
      "Epoch 39, Loss: 0.19167853891849518\n",
      "Epoch 40, Loss: 0.19001176953315735\n",
      "Epoch 41, Loss: 0.19218072295188904\n",
      "Epoch 42, Loss: 0.19362162053585052\n",
      "Epoch 43, Loss: 0.18740332126617432\n",
      "Epoch 44, Loss: 0.18471626937389374\n",
      "Epoch 45, Loss: 0.19013768434524536\n",
      "Epoch 46, Loss: 0.1849825233221054\n",
      "Epoch 47, Loss: 0.19421455264091492\n",
      "Epoch 48, Loss: 0.18871048092842102\n",
      "Epoch 49, Loss: 0.18254686892032623\n",
      "Epoch 50, Loss: 0.18720093369483948\n",
      "Finished Training\n",
      "Test Accuracy: 84.32615124630334%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "# Adapter la taille des datasets\n",
    "df_test = CustomDataset(image_paths_test, labels_test, transform=transformation)\n",
    "test_dataset_size = len(df_test)\n",
    "split_sizes = [test_dataset_size // 2, test_dataset_size - test_dataset_size // 2]\n",
    "test_online, test_dataset = random_split(df_test, split_sizes)\n",
    "test_online_indices = test_online.indices[:300]\n",
    "test_online = torch.utils.data.Subset(df_test, test_online_indices)\n",
    "\n",
    "\n",
    "online_loader = DataLoader(test_online, batch_size = 1024, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "# 1. Charger le modèle ResNet18 pré-entraîné\n",
    "model = resnet18(pretrained=False)\n",
    "\n",
    "# Adapter le modèle pour une sortie unique (classification binaire)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_ftrs, 2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "# 2. Définir le critère et l'optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "                model.parameters(), lr=0.01, weight_decay=0.01\n",
    "            )\n",
    "sched = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# 3. Déplacer le modèle vers le GPU si disponible\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training phase using online_loader\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(online_loader, 0):\n",
    "        inputs, labels = inputs.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    sched.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(online_loader)}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# 4. Testing on testloader\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (inputs, labels) in enumerate(testloader, 0):\n",
    "        inputs, labels = inputs.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        labels_ = torch.argmax(labels, dim=1)\n",
    "        correct_test += (predicted_labels == labels_).sum().item()\n",
    "\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
